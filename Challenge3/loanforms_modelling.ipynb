{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Modelling: From Retrieval to Upload (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will structure the data retrieved from Azure Document Intelligence (ADI) into the right format to be read by our systems in subsequent steps. \n",
    "\n",
    "The data will be outputted from the ADI as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage.\n",
    "\n",
    "As stated before, we need to make sure that our Function will know how to process:\n",
    "- **Loan Forms:** Extract relevant details such as borrower information, loan amounts, and terms.\n",
    "- **Loan Contract:** Identify and parse key contract elements like clauses, signatures, and dates.\n",
    "- **Pay Stubs:** Retrieve data such as employee details, earnings, deductions, and net pay.\n",
    "\n",
    "Not all customers will have provided all types of content, and during this Challenge we will be only be processing one file. We will combine in the next challenge the capabilities of a trigger, which will, at a time, also process one single document.\n",
    "\n",
    "Due to the nature of this challenge, we will separate this challenge in the 3 different types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Forms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to get a Loan, is to fill out a form with some basic details, such as customer ID, Full Name, Date Of Birth, etc, therefore, that's where we will start. \n",
    "\n",
    "This particular document combines text and tables, that the ADI capabilities allow you to extract as also separate capabilities.\n",
    "\n",
    "To first start our analysis, let's create a function that will load the documents inside a folder inside a container that is, on its turn, inside our designated Storage Account. In our particular step, inside the folder of the Loan Forms, we will retrieve one Loan Form for us to analyse. \n",
    "\n",
    "We will consequently use this same function to access other folders that will contain other type of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why are we not batch-analysing documents?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_json_files_from_blob(folder_path):\n",
    "    # Retrieve the connection string from the environment variables\n",
    "    connection_string = os.getenv('STORAGE_CONNECTION_STRING')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(\"data\")\n",
    "\n",
    "    # List all blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "    # Filter out JSON files and read their contents\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith('.json'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            data = json.loads(blob_data)\n",
    "            # print(f\"Contents of {blob.name}:\")\n",
    "            # print(json.dumps(data, indent=2))\n",
    "            # print(\"\\n\")\n",
    "            return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is to call our function and pass the name of our folder as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanform = read_json_files_from_blob(\"loanform\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'handwritten': False, 'pages': [{'page_number': 1, 'width': 8.5, 'height': 11.0, 'unit': 'inch', 'lines': [{'text': 'Loan Application Form PDF', 'polygon': [0.9934, 0.7953, 5.0186, 0.775, 5.0192, 1.082, 0.9955, 1.1017], 'words': [{'content': 'Loan', 'confidence': 0.992}, {'content': 'Application', 'confidence': 0.993}, {'content': 'Form', 'confidence': 0.992}, {'content': 'PDF', 'confidence': 0.997}]}, {'text': 'Applicant Information', 'polygon': [0.9882, 1.5769, 2.7254, 1.5752, 2.7256, 1.7599, 0.9883, 1.7616], 'words': [{'content': 'Applicant', 'confidence': 0.995}, {'content': 'Information', 'confidence': 0.992}]}, {'text': 'Customer ID: 100001', 'polygon': [0.9911, 1.8654, 2.5752, 1.8617, 2.5756, 2.0324, 0.9915, 2.036], 'words': [{'content': 'Customer', 'confidence': 0.995}, {'content': 'ID:', 'confidence': 0.992}, {'content': '100001', 'confidence': 0.997}]}, {'text': 'Full Name: Jane Elizabeth Smith', 'polygon': [0.9899, 2.3156, 3.4199, 2.315, 3.4199, 2.492, 0.9899, 2.4926], 'words': [{'content': 'Full', 'confidence': 0.992}, {'content': 'Name:', 'confidence': 0.995}, {'content': 'Jane', 'confidence': 0.992}, {'content': 'Elizabeth', 'confidence': 0.996}, {'content': 'Smith', 'confidence': 0.998}]}, {'text': 'Date of Birth: 08/22/1990', 'polygon': [0.9906, 2.7696, 2.8563, 2.7658, 2.8567, 2.9433, 0.9908, 2.9472], 'words': [{'content': 'Date', 'confidence': 0.992}, {'content': 'of', 'confidence': 0.997}, {'content': 'Birth:', 'confidence': 0.991}, {'content': '08/22/1990', 'confidence': 0.995}]}, {'text': 'Social Security Number: 987-65-4321', 'polygon': [0.9888, 3.2198, 3.7713, 3.219, 3.7713, 3.4086, 0.9888, 3.4093], 'words': [{'content': 'Social', 'confidence': 0.992}, {'content': 'Security', 'confidence': 0.994}, {'content': 'Number:', 'confidence': 0.994}, {'content': '987-65-4321', 'confidence': 0.993}]}, {'text': 'Contact Number: (555) 234-5678', 'polygon': [0.9906, 3.6768, 3.4488, 3.6755, 3.4489, 3.8607, 0.9907, 3.862], 'words': [{'content': 'Contact', 'confidence': 0.992}, {'content': 'Number:', 'confidence': 0.993}, {'content': '(555)', 'confidence': 0.992}, {'content': '234-5678', 'confidence': 0.995}]}, {'text': 'Email Address: jane.smith90@example.com', 'polygon': [0.9887, 4.1298, 4.2732, 4.1348, 4.2729, 4.3236, 0.9883, 4.3174], 'words': [{'content': 'Email', 'confidence': 0.995}, {'content': 'Address:', 'confidence': 0.993}, {'content': 'jane.smith90@example.com', 'confidence': 0.935}]}, {'text': 'Physical Address: 456 Oak Avenue, Unit 10, Madison, WI 53703', 'polygon': [0.9878, 4.5812, 5.7671, 4.5783, 5.7672, 4.7735, 0.988, 4.7771], 'words': [{'content': 'Physical', 'confidence': 0.992}, {'content': 'Address:', 'confidence': 0.992}, {'content': '456', 'confidence': 0.997}, {'content': 'Oak', 'confidence': 0.996}, {'content': 'Avenue,', 'confidence': 0.994}, {'content': 'Unit', 'confidence': 0.989}, {'content': '10,', 'confidence': 0.995}, {'content': 'Madison,', 'confidence': 0.995}, {'content': 'WI', 'confidence': 0.963}, {'content': '53703', 'confidence': 0.996}]}, {'text': 'Employment and Income Details', 'polygon': [0.989, 5.0371, 3.5666, 5.0351, 3.5668, 5.2313, 0.9892, 5.2333], 'words': [{'content': 'Employment', 'confidence': 0.993}, {'content': 'and', 'confidence': 0.999}, {'content': 'Income', 'confidence': 0.992}, {'content': 'Details', 'confidence': 0.995}]}, {'text': 'Employer', 'polygon': [1.3195, 5.3669, 2.0961, 5.3702, 2.0953, 5.5533, 1.3187, 5.55], 'words': [{'content': 'Employer', 'confidence': 0.997}]}, {'text': 'Name', 'polygon': [1.3196, 5.567, 1.7844, 5.5702, 1.7833, 5.724, 1.3186, 5.7208], 'words': [{'content': 'Name', 'confidence': 0.983}]}, {'text': 'Position', 'polygon': [2.5987, 5.367, 3.2743, 5.3708, 3.2734, 5.5318, 2.5977, 5.5279], 'words': [{'content': 'Position', 'confidence': 0.996}]}, {'text': 'Employment', 'polygon': [3.8739, 5.368, 4.8974, 5.3696, 4.8971, 5.5498, 3.8737, 5.5482], 'words': [{'content': 'Employment', 'confidence': 0.993}]}, {'text': 'Duration', 'polygon': [3.8755, 5.5637, 4.5743, 5.566, 4.5737, 5.7272, 3.8749, 5.7247], 'words': [{'content': 'Duration', 'confidence': 0.997}]}, {'text': 'Monthly', 'polygon': [5.2738, 5.3617, 5.9244, 5.3687, 5.9225, 5.5492, 5.2719, 5.5422], 'words': [{'content': 'Monthly', 'confidence': 0.996}]}, {'text': 'Income', 'polygon': [5.2738, 5.5673, 5.8738, 5.5702, 5.8731, 5.7244, 5.2731, 5.7216], 'words': [{'content': 'Income', 'confidence': 0.992}]}, {'text': 'Contact', 'polygon': [6.5477, 5.3683, 7.183, 5.369, 7.1828, 5.531, 6.5476, 5.5303], 'words': [{'content': 'Contact', 'confidence': 0.996}]}, {'text': 'Number', 'polygon': [6.5464, 5.5622, 7.1938, 5.5634, 7.1935, 5.724, 6.5462, 5.7228], 'words': [{'content': 'Number', 'confidence': 0.997}]}, {'text': 'Horizon', 'polygon': [1.0686, 5.9262, 1.6521, 5.9293, 1.6512, 6.093, 1.068, 6.0899], 'words': [{'content': 'Horizon', 'confidence': 0.998}]}, {'text': 'Store Manager', 'polygon': [2.3509, 5.9235, 3.4752, 5.9283, 3.4743, 6.1117, 2.35, 6.1068], 'words': [{'content': 'Store', 'confidence': 0.995}, {'content': 'Manager', 'confidence': 0.998}]}, {'text': '3 years', 'polygon': [3.6221, 5.9306, 4.1866, 5.9336, 4.1856, 6.1104, 3.6211, 6.1074], 'words': [{'content': '3', 'confidence': 0.995}, {'content': 'years', 'confidence': 0.995}]}, {'text': '$4,583.33', 'polygon': [5.0227, 5.9232, 5.7804, 5.9207, 5.781, 6.0981, 5.0233, 6.1005], 'words': [{'content': '$4,583.33', 'confidence': 0.994}]}, {'text': '(555) 789-', 'polygon': [6.2994, 5.9219, 7.0736, 5.918, 7.0745, 6.1019, 6.3003, 6.1058], 'words': [{'content': '(555)', 'confidence': 0.996}, {'content': '789-', 'confidence': 0.989}]}, {'text': 'Retailers', 'polygon': [1.0694, 6.2123, 1.7405, 6.2161, 1.7395, 6.381, 1.0685, 6.3771], 'words': [{'content': 'Retailers', 'confidence': 0.994}]}, {'text': '2345', 'polygon': [6.2942, 6.213, 6.6793, 6.2129, 6.6793, 6.3752, 6.2942, 6.3753], 'words': [{'content': '2345', 'confidence': 0.991}]}, {'text': 'Loan Information', 'polygon': [0.9885, 7.0337, 2.3641, 7.0346, 2.364, 7.2109, 0.9883, 7.21], 'words': [{'content': 'Loan', 'confidence': 0.992}, {'content': 'Information', 'confidence': 0.995}]}, {'text': 'Loan Amount Requested: $30,000', 'polygon': [0.9886, 7.4825, 3.5383, 7.4804, 3.5385, 7.6738, 0.9888, 7.6759], 'words': [{'content': 'Loan', 'confidence': 0.991}, {'content': 'Amount', 'confidence': 0.994}, {'content': 'Requested:', 'confidence': 0.992}, {'content': '$30,000', 'confidence': 0.994}]}, {'text': 'Purpose of Loan: Vehicle Purchase', 'polygon': [0.9878, 7.9411, 3.6178, 7.9396, 3.6179, 8.1276, 0.9879, 8.1291], 'words': [{'content': 'Purpose', 'confidence': 0.995}, {'content': 'of', 'confidence': 0.992}, {'content': 'Loan:', 'confidence': 0.993}, {'content': 'Vehicle', 'confidence': 0.995}, {'content': 'Purchase', 'confidence': 0.995}]}, {'text': 'Loan Term Desired: 5 years', 'polygon': [0.9872, 8.3941, 3.0692, 8.4019, 3.0685, 8.5839, 0.9865, 8.5761], 'words': [{'content': 'Loan', 'confidence': 0.992}, {'content': 'Term', 'confidence': 0.992}, {'content': 'Desired:', 'confidence': 0.992}, {'content': '5', 'confidence': 0.994}, {'content': 'years', 'confidence': 0.996}]}, {'text': \"Applicant's Signature:\", 'polygon': [0.9877, 8.9982, 2.7554, 9.0006, 2.7552, 9.1906, 0.9874, 9.1882], 'words': [{'content': \"Applicant's\", 'confidence': 0.992}, {'content': 'Signature:', 'confidence': 0.992}]}, {'text': 'Jz', 'polygon': [1.3843, 9.2451, 2.5332, 9.2574, 2.5132, 10.3855, 1.3643, 10.3676], 'words': [{'content': 'Jz', 'confidence': 0.601}]}], 'selection_marks': []}], 'tables': [{'row_count': 3, 'column_count': 5, 'bounding_regions': [{'page_number': 1, 'polygon': [0.9848, 5.3447, 7.5083, 5.3455, 7.5097, 6.5249, 0.9861, 6.5244]}], 'cells': [{'row_index': 0, 'column_index': 0, 'content': 'Employer Name', 'bounding_regions': [{'page_number': 1, 'polygon': [1.0081, 5.3439, 2.2854, 5.3439, 2.2854, 5.734, 1.0081, 5.734]}]}, {'row_index': 0, 'column_index': 1, 'content': 'Position', 'bounding_regions': [{'page_number': 1, 'polygon': [2.2854, 5.3439, 3.556, 5.3439, 3.556, 5.7407, 2.2854, 5.734]}]}, {'row_index': 0, 'column_index': 2, 'content': 'Employment Duration', 'bounding_regions': [{'page_number': 1, 'polygon': [3.556, 5.3439, 4.9611, 5.3506, 4.9611, 5.7407, 3.556, 5.7407]}]}, {'row_index': 0, 'column_index': 3, 'content': 'Monthly Income', 'bounding_regions': [{'page_number': 1, 'polygon': [4.9611, 5.3506, 6.2182, 5.3506, 6.2249, 5.7407, 4.9611, 5.7407]}]}, {'row_index': 0, 'column_index': 4, 'content': 'Contact Number', 'bounding_regions': [{'page_number': 1, 'polygon': [6.2182, 5.3506, 7.4888, 5.3506, 7.4888, 5.7407, 6.2249, 5.7407]}]}, {'row_index': 1, 'column_index': 0, 'content': 'Horizon', 'bounding_regions': [{'page_number': 1, 'polygon': [1.0081, 5.734, 2.2854, 5.734, 2.2787, 6.1645, 1.0081, 6.1645]}]}, {'row_index': 1, 'column_index': 1, 'content': 'Store Manager', 'bounding_regions': [{'page_number': 1, 'polygon': [2.2854, 5.734, 3.556, 5.7407, 3.556, 6.1645, 2.2787, 6.1645]}]}, {'row_index': 1, 'column_index': 2, 'content': '3 years', 'bounding_regions': [{'page_number': 1, 'polygon': [3.556, 5.7407, 4.9611, 5.7407, 4.9611, 6.1645, 3.556, 6.1645]}]}, {'row_index': 1, 'column_index': 3, 'content': '$4,583.33', 'bounding_regions': [{'page_number': 1, 'polygon': [4.9611, 5.7407, 6.2249, 5.7407, 6.2249, 6.1645, 4.9611, 6.1645]}]}, {'row_index': 1, 'column_index': 4, 'content': '(555) 789-', 'bounding_regions': [{'page_number': 1, 'polygon': [6.2249, 5.7407, 7.4888, 5.7407, 7.4888, 6.1645, 6.2249, 6.1645]}]}, {'row_index': 2, 'column_index': 0, 'content': 'Retailers', 'bounding_regions': [{'page_number': 1, 'polygon': [1.0081, 6.1645, 2.2787, 6.1645, 2.2787, 6.5142, 1.0013, 6.5142]}]}, {'row_index': 2, 'column_index': 1, 'content': '', 'bounding_regions': [{'page_number': 1, 'polygon': [2.2787, 6.1645, 3.556, 6.1645, 3.556, 6.5142, 2.2787, 6.5142]}]}, {'row_index': 2, 'column_index': 2, 'content': '', 'bounding_regions': [{'page_number': 1, 'polygon': [3.556, 6.1645, 4.9611, 6.1645, 4.9611, 6.5142, 3.556, 6.5142]}]}, {'row_index': 2, 'column_index': 3, 'content': '', 'bounding_regions': [{'page_number': 1, 'polygon': [4.9611, 6.1645, 6.2249, 6.1645, 6.2249, 6.5142, 4.9611, 6.5142]}]}, {'row_index': 2, 'column_index': 4, 'content': '2345', 'bounding_regions': [{'page_number': 1, 'polygon': [6.2249, 6.1645, 7.4888, 6.1645, 7.4955, 6.5142, 6.2249, 6.5142]}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(loanform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a function that will process the loan application form data. This function will take the loan application form data as input and return the result of the loan application processing. Our input data is a JSON file that is composed of both text and tables, and we will need to treat both of them seperatly.  \n",
    "\n",
    "The function will perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_structured_tables function processes a list of tables by initializing and populating them with cell content, combining specific rows for tables with 3 rows and 5 columns, and returning the structured tables along with any combined rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    combined_rows = []\n",
    "    \n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        # Combine the last row with the previous one if the table has 5 columns and 3 rows\n",
    "        if row_count == 3 and column_count == 5:\n",
    "            combined_row = [structured_table[1][i] + \" \" + structured_table[2][i] for i in range(column_count)]\n",
    "            structured_table[1] = combined_row\n",
    "            structured_table = structured_table[:2]\n",
    "            combined_rows.append(combined_row)\n",
    "        \n",
    "        # Append the structured table to the list\n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables, combined_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean_form_recognizer_result function processes form recognizer output by extracting text data while ignoring lines containing the word \"table\", retaining only the \"text\" key in each line, and creating structured tables from the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_data = []\n",
    "    table_encountered = False\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                table_encountered = True\n",
    "                continue  # Skip the line if \"table\" is in the text\n",
    "            \n",
    "            if not table_encountered:\n",
    "                # Collect the \"text\" information\n",
    "                text_data.append(line.get(\"text\", \"\"))\n",
    "            \n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables, combined_rows = create_structured_tables(data.get(\"tables\", []))\n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"combined_rows\"] = combined_rows\n",
    "    data[\"text_data\"] = text_data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables_to_dataframes function converts a list of structured tables into a list of pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now retrieved both our table with the structured desired and the text that comes out of our files. However, this function doesn't have the data as structured as we need it to be. \n",
    "\n",
    "As an example, we have by now extracted a key-value pair which keys is \"text\" with the value \"Contact Number: (555) 234-5678\". What we will need to define now is to remove the name of the field, and start composing the key-value pair that would be key \"Contact Number:\" and value \"(555) 234-5678\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted to JSON object:\n",
      "[\n",
      "    {\n",
      "        \"id\": \"100001\",\n",
      "        \"Full Name\": \"Jane Elizabeth Smith\",\n",
      "        \"Date of Birth\": \"08/22/1990\",\n",
      "        \"Social Security Number\": \"987-65-4321\",\n",
      "        \"Contact Number\": \"(555) 234-5678\",\n",
      "        \"Email Address\": \"jane.smith90@example.com\",\n",
      "        \"Physical Address\": \"456 Oak Avenue Unit 10 Madison WI 53703\",\n",
      "        \"Loan Amount Requested\": \"30000\",\n",
      "        \"Purpose of Loan\": \"Vehicle Purchase\",\n",
      "        \"Loan Term Desired\": \"5 years Applicant's Signature: Jz\",\n",
      "        \"Employer Name\": \"Horizon Retailers\",\n",
      "        \"Position\": \"Store Manager \",\n",
      "        \"Employment Duration\": \"3 years \",\n",
      "        \"Monthly Income\": \"$4583.33 \",\n",
      "        \"Employer Contact Number\": \"(555) 789- 2345\"\n",
      "    }\n",
      "]\n",
      "Extracted id: 100001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "def clean_loan_application_file(text):\n",
    "    cleaned_data = {}\n",
    "\n",
    "    # Extract the category from the first three words\n",
    "    category_match = re.search(r'(\\w+\\s+\\w+\\s+\\w+)', text)\n",
    "    if category_match:\n",
    "        cleaned_data['Category'] = category_match.group(1)\n",
    "    \n",
    "    # Extract Applicant Information\n",
    "    applicant_info = re.search(r'Applicant Information(.*?)Employment and Income Details', text, re.DOTALL)\n",
    "    if applicant_info:\n",
    "        applicant_info_text = applicant_info.group(1)\n",
    "        cleaned_data['Applicant Information'] = {\n",
    "            'id': re.search(r'Customer ID:\\s*(.*?)Full Name:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Full Name': re.search(r'Full Name:\\s*(.*?)Date of Birth:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Date of Birth': re.search(r'Date of Birth:\\s*(.*?)Social Security Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Social Security Number': re.search(r'Social Security Number:\\s*(.*?)Contact Number:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Contact Number': re.search(r'Contact Number:\\s*(.*?)Email Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Email Address': re.search(r'Email Address:\\s*(.*?)Physical Address:', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Physical Address': re.search(r'Physical Address:\\s*(.*)', applicant_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    # Extract Loan Information\n",
    "    loan_info = re.search(r'Loan Information(.*)', text, re.DOTALL)\n",
    "    if loan_info:\n",
    "        loan_info_text = loan_info.group(1)\n",
    "        cleaned_data['Loan Information'] = {\n",
    "            'Loan Amount Requested': re.search(r'Loan Amount Requested:\\s*\\$?(.*?)Purpose of Loan:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Purpose of Loan': re.search(r'Purpose of Loan:\\s*(.*?)Loan Term Desired:', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "            'Loan Term Desired': re.search(r'Loan Term Desired:\\s*(.*)', loan_info_text, re.DOTALL).group(1).strip(),\n",
    "        }\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to combine extracted tables and text\n",
    "def process_loan_application(data):\n",
    "    # Clean form recognizer result to extract structured tables and text\n",
    "    cleaned_data = clean_form_recognizer_result(data)\n",
    "    \n",
    "    # Convert extracted tables to dataframes\n",
    "    dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"]) \n",
    "    # Combine all table dataframes into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) \n",
    "    combined_df.columns = combined_df.iloc[0]\n",
    "    combined_df = combined_df[1:]\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    combined_df.rename(columns={\"Contact Number\": \"Employer Contact Number\"}, inplace=True)\n",
    "    combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "    # Clean the extracted text using regex\n",
    "    combined_text = ' '.join(cleaned_data['text_data'])\n",
    "    text_data = clean_loan_application_file(combined_text)\n",
    "\n",
    "    def clean_loan_application(data):\n",
    "    # Extract applicant and loan info\n",
    "        applicant_info = data['Applicant Information']\n",
    "        loan_info = data['Loan Information']\n",
    "        \n",
    "        # Combine keys and values for the two categories\n",
    "        fields = list(applicant_info.keys()) + list(loan_info.keys())\n",
    "        values = list(applicant_info.values()) + list(loan_info.values())\n",
    "        \n",
    "        # Create the 2x10 DataFrame without 'Category'\n",
    "        df = pd.DataFrame({\n",
    "            'Field': fields,\n",
    "            'Value': values\n",
    "        })\n",
    "        \n",
    "        return df.set_index('Field').T\n",
    "\n",
    "    df_cleaned = clean_loan_application(text_data)\n",
    "\n",
    "    # Convert the text data to a DataFrame\n",
    "    text_df = pd.DataFrame(df_cleaned)\n",
    "\n",
    "    # Concatenate the text dataframe with the tables dataframe\n",
    "    final_df = pd.concat([text_df, combined_df], axis=1)\n",
    "\n",
    "    def remove_empty_cells_and_push_up(df):\n",
    "        for column in df.columns:\n",
    "            non_empty_values = df[column].replace('', pd.NA).dropna().values\n",
    "            df[column] = pd.Series(non_empty_values).reindex(df.index, fill_value='')\n",
    "        return df\n",
    "    return remove_empty_cells_and_push_up(final_df)\n",
    "\n",
    "# Process the loan application\n",
    "loanform_structured = process_loan_application(loanform).iloc[1:].reset_index(drop=True)\n",
    "loanform_structured.replace(\"Applicant's Signature:,\", '', regex=True, inplace=True)\n",
    "loanform_structured.replace(\"\\,\", '', regex=True,  inplace=True)\n",
    "\n",
    "# Convert DataFrame to JSON\n",
    "json_loanform = loanform_structured.to_json(orient=\"records\")\n",
    "\n",
    "# Convert JSON string to a Python dictionary\n",
    "data = json.loads(json_loanform)\n",
    "\n",
    "#Step 1: Remove unwanted characters (if necessary)\n",
    "cleaned_json = json_loanform.strip()\n",
    "\n",
    "# Step 2: Replace escaped characters\n",
    "cleaned_json = cleaned_json.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
    "\n",
    "# Step 3: Load the cleaned string into a JSON object\n",
    "try:\n",
    "    json_data = json.loads(cleaned_json)\n",
    "    print(\"Successfully converted to JSON object:\")\n",
    "    print(json.dumps(json_data, indent=4))\n",
    "    customer_id = json_data[0].get('id')\n",
    "    print(f\"Extracted id: {customer_id}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    print(\"Cleaned JSON string:\")\n",
    "    print(cleaned_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Architecturing: From Retrieval to Upload (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Cosmos DB connection details from environment variables\n",
    "endpoint = os.getenv(\"COSMOS_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_KEY\")\n",
    "\n",
    "def upload_text_to_cosmos_db(text_content, container_name):\n",
    "    # Check if the text is empty\n",
    "    if not text_content:\n",
    "        print(\"The text content is empty. No data to upload.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    try:\n",
    "        # Create or get the database\n",
    "        database = client.create_database_if_not_exists(id=\"ContosoDB\")\n",
    "        \n",
    "        # Create or get the container\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path=f\"/id\"),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while creating the database or container: {e.message}\")\n",
    "        return\n",
    "    \n",
    "    # Create a document with the text content and partition key\n",
    "    document = {\n",
    "        'id': str(customer_id),  # Generate a unique ID for the document\n",
    "        'content': text_content,  # Store the plain text as 'content'\n",
    "    }\n",
    "    \n",
    "    # Upload the document to the container\n",
    "    try:\n",
    "        container.create_item(body=document)\n",
    "        print(f\"Text content uploaded successfully with ID '{document['id']}' in Cosmos DB.\")\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while uploading the document: {e.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Loan Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content uploaded successfully with ID '100001' in Cosmos DB.\n"
     ]
    }
   ],
   "source": [
    "upload_text_to_cosmos_db(json_data, \"LoanForms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
